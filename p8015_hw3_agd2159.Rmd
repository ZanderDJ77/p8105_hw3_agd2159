---
title: "P8015_Hw3_agd2159"
output: github_document
author: "Zander De Jesus"
date: "10-14-2023"
---
# Problem 1: Instacart Grocery Large-Scale Data Analysis and Plotting

Begin by loading important libraries, organizing markdown, and beginning to pull in full dataset for exploratory data analysis. 

```{r}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)

data("instacart")

summary(instacart)

nrow(instacart)
ncol(instacart)

```
This is a large dataset that contains `r nrow(instacart)` rows of observations and `r ncol(instacart)` column variables. Each row is a specific instacart product - with associated department and aisle information. There are also categorical variables that describe the time and day of the order, and number of day since the previous order. 

In total, there are `r instacart |> select(product_id) |> distinct() |> count()` products found in `r instacart |> select(user_id, order_id) |> distinct() |> count()` orders from `r instacart |> select(user_id) |> distinct() |> count()` distinct users.

### Task 1 - How many aisles are there, and which aisles are the most items ordered from?**

```{r}

num = instacart |> 
  distinct(aisle_id) |> 
  max()

aisles_df = instacart |> 
  janitor::clean_names() |> 
  group_by(department, aisle) |> 
  summarize(n_obs = n()) |> 
  arrange(desc(n_obs))

aisles_df

```
There are **134 distinct aisles** across all the products instacart offers.
After creating a separate grouped tibble based on aisle name and the bulk total of orders from each aisle, we can see that the the top 3 aisles with the highest amount of items ordered are **Fresh Vegetables, Fresh Fruits, and Packaged Vegetables / Fruits.** These are then followed by Yogurt and Packaged Cheese in the dairy department. 

### Task 2 - Plotting Number of Items sold in Highest Demand Aisles**

```{r}
aisles_df |> 
  filter(n_obs >= 10000) |> 
  mutate(aisle = fct_reorder(aisle, n_obs)) |> 
  ggplot(aes(x=aisle, y = n_obs, fill = department)) + 
  geom_col() +
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle= 60, hjust = 1))

```
This column graph visualizes the relative quantities of items ordered across each aisle, color-coded by associated department. Internal to each department, the aisles are organized from least in demeand to most in demand items.

### Task 3 - Identifying Most Popular Items in Specific Aisles**

```{r}

popular_items = instacart |> 
  janitor::clean_names() |> 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") |> 
  group_by(aisle, product_name) |> 
  summarize(n_obs = n()) |> 
  arrange(desc(n_obs)) |> 
  mutate(item_rank = min_rank(desc(n_obs))) |> 
  filter(item_rank < 4)
           
popular_items |> 
    knitr::kable(digits = 2)

```

This table above provides the top three most popular items in the requested aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`. Because of the relative demand for produce across all orders, the packaged vegetable fruit orders are all much higher than the other two aisles of interest.

### Task 4 - Looking at weekly orders of two products

```{r}
Apple_Coffee = instacart |> 
  select(product_name, order_dow, order_hour_of_day) |> 
  group_by(product_name, order_dow) |> 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") |> 
  mutate(
    order_dow = recode(
      order_dow,
      "0" = "Sunday",
      "1" = "Monday",
      "2" = "Tuesday",
      "3" = "Wednesday",
      "4" = "Thursday",
      "5" = "Friday",
      "6" = "Saturday")) |> 
  summarize(avg_hr = mean(order_hour_of_day)) |> 
  pivot_wider(
    names_from = order_dow,
    values_from = avg_hr)

Apple_Coffee |> 
    knitr::kable(digits = 2)
  
```

This final part of Problem 1 asks us to narrow the dataset down to just two procucts - Pink Lady Apples and Coffee Ice Cream, and look at there average times across the week. General trends show that during the work week, Apples are usually ordered earlier for lunch or snack while ice cream is ordered on average after work in the evening. Meanwhlile on the weekend Fri through Saturday, Ice cream and Apples have similar order times, being bought in the mid afternoon 12 to 1 pm for each product.


# Problem 2: Behavioral Risk Factors Dataset Analysis
Beginning by importing the dataset and doing initial steps to clean the tibble dataframe.
### Task 1: Initial Data Cleaning
```{r}
data("brfss_smart2010")

brfss_clean = brfss_smart2010 |> 
  janitor::clean_names() |> 
  select(everything(), -data_value_unit, -data_value_footnote, -data_value_footnote_symbol, -location_id) |> 
  rename(state = locationabbr, county = locationdesc) |> 
  filter(topic == "Overall Health")

#Specific code to reorder response factor levels
brfss_clean = brfss_clean |> 
  mutate(response = as.factor(response)) |> 
  mutate(response = ordered(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
```
The original dataset before cleaning has `r nrow(brfss_smart2010)` rows of content, indicating a different question category from each state location, and `r ncol(brfss_smart2010)` column variables. 3 of these columns were entirely empty with NA's and were removed in the tidy dataset `brfss_clean`. Also one column `data_value_unit` was only describing that the unit was in percentages, and was removed to avoid confusion in calculations. 

For clarity, locationabbr and locationdesc were renamed to state and county respectively. After filtering only the Overall Health topic questions, and removign the extraneous columns identified above, this cleaned dataframe has `r nrow(brfss_clean)` rows and `r ncol(brfss_clean)` column variables.

The `response` column was converted from character to factor format, with five levels ordered from Poor -> Excellent as requested. 

```{r}

test = brfss_clean |> 
  pull(response)
```

### Task 2: Analyzing Specific Response Data by Years
*Q: In 2002, which states were observed at 7 or more locations? What about in 2010?*
To do this, we will make two filtered subdataframes for both 2002 and 2010, that make tables containing 7 or more county locations for each state. This will use the `group_by` function. 

```{r}

```




