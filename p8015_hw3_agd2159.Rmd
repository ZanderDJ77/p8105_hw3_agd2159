---
title: "P8015_Hw3_agd2159"
output: github_document
author: "Zander De Jesus"
date: "10-14-2023"
---
# Problem 1: Instacart Grocery Large-Scale Data Analysis and Plotting

Begin by loading important libraries, organizing markdown, and beginning to pull in full dataset for exploratory data analysis. 

```{r}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)

data("instacart")

summary(instacart)

nrow(instacart)
ncol(instacart)

```
This is a large dataset that contains `r nrow(instacart)` rows of observations and `r ncol(instacart)` column variables. Each row is a specific instacart product - with associated department and aisle information. There are also categorical variables that describe the time and day of the order, and number of day since the previous order. 

In total, there are `r instacart |> select(product_id) |> distinct() |> count()` products found in `r instacart |> select(user_id, order_id) |> distinct() |> count()` orders from `r instacart |> select(user_id) |> distinct() |> count()` distinct users.

### Task 1 - How many aisles are there, and which aisles are the most items ordered from?**

```{r}

num = instacart |> 
  distinct(aisle_id) |> 
  max()

aisles_df = instacart |> 
  janitor::clean_names() |> 
  group_by(department, aisle) |> 
  summarize(n_obs = n()) |> 
  arrange(desc(n_obs))

aisles_df

```
There are **134 distinct aisles** across all the products instacart offers.
After creating a separate grouped tibble based on aisle name and the bulk total of orders from each aisle, we can see that the the top 3 aisles with the highest amount of items ordered are **Fresh Vegetables, Fresh Fruits, and Packaged Vegetables / Fruits.** These are then followed by Yogurt and Packaged Cheese in the dairy department. 

### Task 2 - Plotting Number of Items sold in Highest Demand Aisles**

```{r}
aisles_df |> 
  filter(n_obs >= 10000) |> 
  mutate(aisle = fct_reorder(aisle, n_obs)) |> 
  ggplot(aes(x=aisle, y = n_obs, fill = department)) + 
  geom_col() +
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle= 60, hjust = 1))

```
This column graph visualizes the relative quantities of items ordered across each aisle, color-coded by associated department. Internal to each department, the aisles are organized from least in demeand to most in demand items.

### Task 3 - Identifying Most Popular Items in Specific Aisles**

```{r}

popular_items = instacart |> 
  janitor::clean_names() |> 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") |> 
  group_by(aisle, product_name) |> 
  summarize(n_obs = n()) |> 
  arrange(desc(n_obs)) |> 
  mutate(item_rank = min_rank(desc(n_obs))) |> 
  filter(item_rank < 4)
           
popular_items |> 
    knitr::kable(digits = 2)

```

This table above provides the top three most popular items in the requested aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`. Because of the relative demand for produce across all orders, the packaged vegetable fruit orders are all much higher than the other two aisles of interest.

### Task 4 - Looking at weekly orders of two products

```{r}
Apple_Coffee = instacart |> 
  select(product_name, order_dow, order_hour_of_day) |> 
  group_by(product_name, order_dow) |> 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") |> 
  mutate(
    order_dow = recode(
      order_dow,
      "0" = "Sunday",
      "1" = "Monday",
      "2" = "Tuesday",
      "3" = "Wednesday",
      "4" = "Thursday",
      "5" = "Friday",
      "6" = "Saturday")) |> 
  summarize(avg_hr = mean(order_hour_of_day)) |> 
  pivot_wider(
    names_from = order_dow,
    values_from = avg_hr)

Apple_Coffee |> 
    knitr::kable(digits = 2)
  
```

This final part of Problem 1 asks us to narrow the dataset down to just two procucts - Pink Lady Apples and Coffee Ice Cream, and look at there average times across the week. General trends show that during the work week, Apples are usually ordered earlier for lunch or snack while ice cream is ordered on average after work in the evening. Meanwhlile on the weekend Fri through Saturday, Ice cream and Apples have similar order times, being bought in the mid afternoon 12 to 1 pm for each product.


# Problem 2: Behavioral Risk Factors Dataset Analysis
Beginning by importing the dataset and doing initial steps to clean the tibble dataframe.
### Task 1 - Initial Data Cleaning
```{r}
data("brfss_smart2010")

brfss_clean = brfss_smart2010 |> 
  janitor::clean_names() |> 
  select(everything(), -data_value_unit, -data_value_footnote, -data_value_footnote_symbol, -location_id) |> 
  rename(state = locationabbr, county = locationdesc) |> 
  filter(topic == "Overall Health")

#Specific code to reorder response factor levels
brfss_clean = brfss_clean |> 
  mutate(response = as.factor(response)) |> 
  mutate(response = ordered(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
```
The original dataset before cleaning has `r nrow(brfss_smart2010)` rows of content, indicating a different question category from each state location, and `r ncol(brfss_smart2010)` column variables. 3 of these columns were entirely empty with NA's and were removed in the tidy dataset `brfss_clean`. Also one column `data_value_unit` was only describing that the unit was in percentages, and was removed to avoid confusion in calculations. 

For clarity, locationabbr and locationdesc were renamed to state and county respectively. After filtering only the Overall Health topic questions, and removign the extraneous columns identified above, this cleaned dataframe has `r nrow(brfss_clean)` rows and `r ncol(brfss_clean)` column variables.

The `response` column was converted from character to factor format, with five levels ordered from Poor -> Excellent as requested. 

### Task 2 - Analyzing Specific Response Data by Years

*Q: In 2002, which states were observed at 7 or more locations? What about in 2010?*

To do this, we will make two filtered subdataframes for both 2002 and 2010, that make tables containing 7 or more county locations for each state. This will use the `group_by` function. 

```{r}

largestates_2002 = brfss_clean |> 
  filter(year == 2002) |> 
  group_by(year, state) |> 
  summarize(n_locations = (n()/5)) |> 
  filter(n_locations >= 7) |> 
  arrange(desc(n_locations))
  

largestates_2002 |> 
  knitr::kable(digits = 2)
```
Because each distinct county will have the 5 levels of the response column for 1 entry, a state with 7 or more county locations can be identified by having 35 or more response entry levels.

In 2002, Six states - **Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania** - had responses coming from 7 or more distinct county locations. Pennsylvania had the highest capacity at 10 different locations (50/5 responses per location).

Below does the same exploratory data organization for the year 2010:

```{r}
largestates_2010 = brfss_clean |> 
  filter(year == 2010) |> 
  group_by(year, state) |> 
  summarize(n_locations = (n()/5)) |> 
  filter(n_locations >= 7) |> 
  arrange(desc(n_locations))

largestates_2010 |> 
  knitr::kable(digits = 2)
```
By 2010, we can see that **14 different states** have 7 or more distinct county locations covered by the response survey to Overall Health. The state with the most county representation in this year was Florida, with 41 distinct county locations reporting responses (205/5 response levels per location).

### Task 3 - Visualizing Excellent Health Responses across States over Time.

```{r}
#Organizing Dataframe
compare_excellent = brfss_clean |> 
  select(year, state, county, response, data_value) |> 
  filter(response == "Excellent") |> 
  group_by(year, state) |> 
  summarize(mean_excellent = mean(data_value)) 

compare_excellent

```

```{r}
#Creating Spaghetti Plot
compare_excellent |> 
  ggplot(aes(x = year, y = mean_excellent, group = state)) + geom_smooth(se = FALSE)
  
```
Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

#### Task 4 - Health Response Distribution across NY State
*Q: Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.*

```{r}

```


# Problem 3: NHANES Accelerometer Data Analysis


